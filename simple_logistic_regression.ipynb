{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BNCBcJ9f500_"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfLM55lq4oe8",
        "outputId": "51c89391-a318-4405-e992-45254895144d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0,loss nan\n",
            "\n",
            "wight [0.22297356],bais[0.46680826]\n",
            "epoch 100,loss nan\n",
            "\n",
            "wight [-0.08369311],bais[0.25347493]\n",
            "epoch 200,loss nan\n",
            "\n",
            "wight [-0.05035978],bais[0.15347493]\n",
            "epoch 300,loss nan\n",
            "\n",
            "wight [-0.01702644],bais[0.05347493]\n",
            "epoch 400,loss nan\n",
            "\n",
            "wight [0.00297356],bais[0.01680826]\n",
            "epoch 500,loss nan\n",
            "\n",
            "wight [-0.00369311],bais[0.01347493]\n",
            "epoch 600,loss nan\n",
            "\n",
            "wight [-0.01035978],bais[0.0101416]\n",
            "epoch 700,loss nan\n",
            "\n",
            "wight [0.00297356],bais[0.01680826]\n",
            "epoch 800,loss nan\n",
            "\n",
            "wight [-0.00369311],bais[0.01347493]\n",
            "epoch 900,loss nan\n",
            "\n",
            "wight [-0.01035978],bais[0.0101416]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-5f6cf352793f>:16: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = -np.sum(y_targ * np.log(y_pred) + (1 - y_targ) * np.log(1 - y_pred))\n",
            "<ipython-input-6-5f6cf352793f>:16: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(y_targ * np.log(y_pred) + (1 - y_targ) * np.log(1 - y_pred))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class simple_logistic_regression():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w = np.random.randn(1)\n",
        "        self.b = np.random.randn(1)\n",
        "\n",
        "    def predict(self,x):\n",
        "     z=np.dot(x,self.w) + self.b\n",
        "     f_ln=1/(1+np.exp(-z))\n",
        "     f_ln=(f_ln>=0.5).astype(int)\n",
        "\n",
        "     return f_ln\n",
        "\n",
        "    def loss(self ,y_pred, y_targ):\n",
        "      m = x.shape[0]\n",
        "      loss = -np.sum(y_targ * np.log(y_pred) + (1 - y_targ) * np.log(1 - y_pred))\n",
        "      cost=loss/(m)\n",
        "      return cost\n",
        "\n",
        "\n",
        "    def gradient_descent(self,lr,y_targ,x):\n",
        "      f_ln = self.predict(x)\n",
        "      cost = self.loss(f_ln, y_targ)\n",
        "\n",
        "      m = x.shape[0]\n",
        "\n",
        "\n",
        "       # Compute gradients\n",
        "      dw = (1 / m) * np.dot(x.T, (f_ln - y_targ))\n",
        "      db = (1 / m) * np.sum(f_ln - y_targ)\n",
        "\n",
        "        # Update weights\n",
        "      self.w = self.w - (lr * dw)\n",
        "      self.b = self.b - (lr * db)\n",
        "\n",
        "\n",
        "   #   db = (1 / m) * np.sum(y_targ - f_ln)\n",
        "      #self.w=self.w -(lr * dw)\n",
        "      #self.b=self.b-(lr * db)\n",
        "      return cost\n",
        "\n",
        "    def fit (self,x,y_targ,lr,epochs):\n",
        "      for i in range(epochs):\n",
        "        loss_value=self.gradient_descent(lr, y_targ,x)\n",
        "        if i%100==0:\n",
        "          print(f\"epoch {i},loss {loss_value}\\n\")\n",
        "          print(f\"wight {self.w},bais{self.b}\")\n",
        "\n",
        "\n",
        "\n",
        "x = np.array([[1], [2], [3]])\n",
        "y = np.array([1, 0, 1])\n",
        "\n",
        "model = simple_logistic_regression()\n",
        "\n",
        "model.fit(x, y, lr=0.01, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LoSV4fuMxSU",
        "outputId": "954a6906-b777-4521-e22c-c7f18a098354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.2045717116341752\n",
            "Weight: [-0.38729945], Bias: [-0.69089659]\n",
            "\n",
            "Epoch 100, Loss: 0.6925413685381003\n",
            "Weight: [0.21246382], Bias: [-0.39154534]\n",
            "\n",
            "Epoch 200, Loss: 0.6546748209728681\n",
            "Weight: [0.36296738], Bias: [-0.29171289]\n",
            "\n",
            "Epoch 300, Loss: 0.6510103862972129\n",
            "Weight: [0.39664941], Bias: [-0.2451049]\n",
            "\n",
            "Epoch 400, Loss: 0.6499871320023374\n",
            "Weight: [0.39875034], Bias: [-0.21366999]\n",
            "\n",
            "Epoch 500, Loss: 0.6492353465557904\n",
            "Weight: [0.39172377], Bias: [-0.18722601]\n",
            "\n",
            "Epoch 600, Loss: 0.6485445469261236\n",
            "Weight: [0.38213732], Bias: [-0.1627657]\n",
            "\n",
            "Epoch 700, Loss: 0.6478933061671092\n",
            "Weight: [0.3719772], Bias: [-0.1393633]\n",
            "\n",
            "Epoch 800, Loss: 0.6472777718424837\n",
            "Weight: [0.36184568], Bias: [-0.11672296]\n",
            "\n",
            "Epoch 900, Loss: 0.6466958343591763\n",
            "Weight: [0.35192157], Bias: [-0.09474191]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class simple_linear_logistic_regression:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w = np.random.randn(1)\n",
        "        self.b = np.random.randn(1)\n",
        "\n",
        "    def loss(self, y_pred, y_targ):\n",
        "        m = y_targ.shape[0]\n",
        "        # Compute the logistic loss\n",
        "        loss = -np.sum(y_targ * np.log(y_pred) + (1 - y_targ) * np.log(1 - y_pred))\n",
        "        cost = loss / m\n",
        "        return cost\n",
        "\n",
        "    def gradient_descent(self, lr, y_targ, x):\n",
        "        m = x.shape[0]\n",
        "        # Prediction\n",
        "        y_pred = self.predict(x)\n",
        "\n",
        "        # Compute the cost\n",
        "        cost = self.loss(y_pred, y_targ)\n",
        "\n",
        "        # Calculate gradients\n",
        "        dw = (1 / m) * np.dot(x.T, (y_pred - y_targ))\n",
        "        db = (1 / m) * np.sum(y_pred - y_targ)\n",
        "\n",
        "        # Update weights\n",
        "        self.w = self.w - lr * dw\n",
        "        self.b = self.b - lr * db\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def fit(self, x, y_targ, lr, epochs):\n",
        "        for i in range(epochs):\n",
        "            loss_value = self.gradient_descent(lr, y_targ, x)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch {i}, Loss: {loss_value}\")\n",
        "                print(f\"Weight: {self.w}, Bias: {self.b}\\n\")\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Logistic function\n",
        "        y_pred = 1 / (1 + np.exp(-(np.dot(x, self.w) + self.b)))\n",
        "        return y_pred\n",
        "\n",
        "# Example usage\n",
        "x = np.array([[1], [2], [3]])\n",
        "y = np.array([1, 0, 1])\n",
        "\n",
        "model = simple_linear_logistic_regression()\n",
        "model.fit(x, y, lr=0.01, epochs=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlY1rWfxJMa_",
        "outputId": "9499a157-4995-4092-c693-a4abbfc629fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.6420778359076145\n",
            "Weight: [0.195229], Bias: [0.15139743]\n",
            "Epoch 100, Loss: 0.6408472652375828\n",
            "Weight: [0.21572601], Bias: [0.17754953]\n",
            "Epoch 200, Loss: 0.6405357895818358\n",
            "Weight: [0.21740838], Bias: [0.19481349]\n",
            "Epoch 300, Loss: 0.6403136024570159\n",
            "Weight: [0.21373629], Bias: [0.20922491]\n",
            "Epoch 400, Loss: 0.6401097546960511\n",
            "Weight: [0.20859753], Bias: [0.22253894]\n",
            "Epoch 500, Loss: 0.6399174752558678\n",
            "Weight: [0.20313591], Bias: [0.23528064]\n",
            "Epoch 600, Loss: 0.6397356252373584\n",
            "Weight: [0.1976876], Bias: [0.24761263]\n",
            "Epoch 700, Loss: 0.6395635907869334\n",
            "Weight: [0.1923493], Bias: [0.25958988]\n",
            "Epoch 800, Loss: 0.6394008324509114\n",
            "Weight: [0.18714667], Bias: [0.27123518]\n",
            "Epoch 900, Loss: 0.6392468440495535\n",
            "Weight: [0.18208439], Bias: [0.2825616]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class simple_linear_logistic_regression:\n",
        "    def __init__(self):\n",
        "        self.w = np.random.randn(1)\n",
        "        self.b = np.random.randn(1)\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(x, self.w) + self.b\n",
        "        f_ln = 1 / (1 + np.exp(-z))\n",
        "        return f_ln\n",
        "\n",
        "    def loss(self, y_pred, y_targ):\n",
        "        m = y_targ.shape[0]\n",
        "        # To avoid log(0), add a small epsilon\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "        loss = -np.sum(y_targ * np.log(y_pred) + (1 - y_targ) * np.log(1 - y_pred))\n",
        "        cost = loss / m\n",
        "        return cost\n",
        "\n",
        "    def gradient_descent(self, lr, y_targ, x):\n",
        "        f_ln = self.predict(x)  # Get probabilities\n",
        "        cost = self.loss(f_ln, y_targ)\n",
        "\n",
        "        m = x.shape[0]\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = (1 / m) * np.dot(x.T, (f_ln - y_targ))\n",
        "        db = (1 / m) * np.sum(f_ln - y_targ)\n",
        "\n",
        "        # Update weights\n",
        "        self.w = self.w - (lr * dw)\n",
        "        self.b = self.b - (lr * db)\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def fit(self, x, y_targ, lr, epochs):\n",
        "        for i in range(epochs):\n",
        "            loss_value = self.gradient_descent(lr, y_targ, x)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch {i}, Loss: {loss_value}\")\n",
        "                print(f\"Weight: {self.w}, Bias: {self.b}\")\n",
        "\n",
        "# Example usage\n",
        "x = np.array([[1], [2], [3]])\n",
        "y = np.array([1, 0, 1])\n",
        "\n",
        "model = simple_linear_logistic_regression()\n",
        "model.fit(x, y, lr=0.01, epochs=1000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
