{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from re import A"
      ],
      "metadata": {
        "id": "6yRbHnM7xuMk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, neurons_each_layer, num_iterations=1000, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initializes the neural network.\n",
        "\n",
        "        Args:\n",
        "            neurons_each_layer (list): Number of neurons in each layer.\n",
        "            num_iterations (int): Number of training iterations.\n",
        "            learning_rate (float): Learning rate for gradient descent.\n",
        "        \"\"\"\n",
        "        self.neurons_each_layer = neurons_each_layer\n",
        "        self.num_iterations = num_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.parameters = self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"Initializes weights and biases for each layer.\"\"\"\n",
        "        parameters = {}\n",
        "        for l in range(1, len(self.neurons_each_layer)):\n",
        "            parameters[f\"W{l}\"] = np.random.randn(self.neurons_each_layer[l], self.neurons_each_layer[l-1]) * 0.01\n",
        "            parameters[f\"b{l}\"] = np.zeros((self.neurons_each_layer[l], 1))\n",
        "        return parameters\n",
        "\n",
        "    def forward(self, activation, A_pre, W, b):\n",
        "        \"\"\"Performs a single forward step for one layer.\"\"\"\n",
        "        Z = np.dot(W, A_pre) + b\n",
        "        linear_cache = (W, A_pre, b)\n",
        "\n",
        "        if activation == \"sigmoid\":\n",
        "            A = 1 / (1 + np.exp(-Z))\n",
        "            activation_cache = Z\n",
        "        elif activation == \"relu\":\n",
        "            A = np.maximum(0, Z)\n",
        "            activation_cache = Z\n",
        "\n",
        "        cache = (linear_cache, activation_cache)\n",
        "        return A, cache\n",
        "\n",
        "    def forward_model(self, X):\n",
        "        \"\"\"Performs forward propagation through the entire network.\"\"\"\n",
        "        caches = []\n",
        "        A = X\n",
        "        L = len(self.parameters) // 2\n",
        "\n",
        "        for l in range(1, L):\n",
        "            A_prev = A\n",
        "            A, cache = self.forward(\"relu\", A_prev, self.parameters[f\"W{l}\"], self.parameters[f\"b{l}\"])\n",
        "            caches.append(cache)\n",
        "\n",
        "        AL, cache = self.forward(\"sigmoid\", A, self.parameters[f\"W{L}\"], self.parameters[f\"b{L}\"])\n",
        "        caches.append(cache)\n",
        "        return AL, caches\n",
        "\n",
        "    def compute_cost(self, Y, AL):\n",
        "        \"\"\"Computes the cost function.\"\"\"\n",
        "        m = Y.shape[1]\n",
        "        loss = Y * np.log(AL) + (1 - Y) * np.log(1 - AL)\n",
        "        cost = -np.sum(loss) / m\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward(self, dA, activation, cache):\n",
        "        \"\"\"Performs backward propagation for a single layer.\"\"\"\n",
        "        linear_cache, activation_cache = cache\n",
        "        W, A_prev, b = linear_cache\n",
        "        m = A_prev.shape[1]\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            dZ = np.array(dA, copy=True)\n",
        "            dZ[activation_cache <= 0] = 0\n",
        "        elif activation == \"sigmoid\":\n",
        "            s = 1 / (1 + np.exp(-activation_cache))\n",
        "            dZ = dA * s * (1 - s)\n",
        "\n",
        "        dW = 1 / m * np.dot(dZ, A_prev.T)\n",
        "        db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "        return dA_prev, dW, db\n",
        "\n",
        "    def backward_model(self, AL, Y, caches):\n",
        "        \"\"\"Performs backward propagation through the entire network.\"\"\"\n",
        "        gradients = {}\n",
        "        L = len(caches)\n",
        "        Y = Y.reshape(AL.shape)\n",
        "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "        current_cache = caches[L-1]\n",
        "        gradients[f\"dA{L-1}\"], gradients[f\"dW{L}\"], gradients[f\"db{L}\"] = self.backward(dAL, \"sigmoid\", current_cache)\n",
        "\n",
        "        for l in reversed(range(L-1)):\n",
        "            current_cache = caches[l]\n",
        "            dA_prev_temp, dW_temp, db_temp = self.backward(gradients[f\"dA{l+1}\"], \"relu\", current_cache)\n",
        "            gradients[f\"dA{l}\"] = dA_prev_temp\n",
        "            gradients[f\"dW{l+1}\"] = dW_temp\n",
        "            gradients[f\"db{l+1}\"] = db_temp\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def update_parameters(self, gradients):\n",
        "        \"\"\"Updates the parameters using gradient descent.\"\"\"\n",
        "        L = len(self.parameters) // 2\n",
        "\n",
        "        for l in range(1, L+1):\n",
        "            self.parameters[f\"W{l}\"] -= self.learning_rate * gradients[f\"dW{l}\"]\n",
        "            self.parameters[f\"b{l}\"] -= self.learning_rate * gradients[f\"db{l}\"]\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        \"\"\"Trains the neural network using forward and backward propagation.\"\"\"\n",
        "        for i in range(self.num_iterations):\n",
        "            AL, caches = self.forward_model(X)\n",
        "            cost = self.compute_cost(Y, AL)\n",
        "            gradients = self.backward_model(AL, Y, caches)\n",
        "            self.update_parameters(gradients)\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Iteration {i}, Cost: {cost}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the output for a given input.\"\"\"\n",
        "        AL, _ = self.forward_model(X)\n",
        "        return (AL > 0.5).astype(int)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gEx5Cmu7NN3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "n_x = 4\n",
        "m = 3\n",
        "np.random.seed(1)\n",
        "X = np.random.randn(n_x, m)\n",
        "Y = np.random.randint(0, 2, size=(1, m))\n",
        "neural_net = NeuralNetwork([n_x, 2, 3, 1], num_iterations=1000, learning_rate=0.01)\n",
        "neural_net.train(X, Y)\n",
        "print(\"Predictions:\", neural_net.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBL0e0GT6_nd",
        "outputId": "afd101a3-d58c-4ee3-b322-09984cf6033d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 0.6931470547312185\n",
            "Iteration 100, Cost: 0.6712612760527906\n",
            "Iteration 200, Cost: 0.6579585894183689\n",
            "Iteration 300, Cost: 0.6498276567493818\n",
            "Iteration 400, Cost: 0.6448247915679844\n",
            "Iteration 500, Cost: 0.6417264705669424\n",
            "Iteration 600, Cost: 0.6397963081568999\n",
            "Iteration 700, Cost: 0.6385877214441823\n",
            "Iteration 800, Cost: 0.6378276885691334\n",
            "Iteration 900, Cost: 0.6373480208978698\n",
            "Predictions: [[0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}