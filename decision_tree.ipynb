{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EoJB2O__Hyvc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from types import coroutine\n",
        "\n",
        "data = np.array([\n",
        "    [1, 0, 1, 1],  # Pointy, Not Round, Whiskers, Cat\n",
        "    [0, 1, 0, 0],  # Floppy, Round, No Whiskers, Not Cat\n",
        "    [1, 1, 1, 1],  # Pointy, Round, Whiskers, Cat\n",
        "    [0, 0, 0, 0]   # Floppy, Not Round, No Whiskers, Not Cat\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "    \"\"\"\n",
        "    Computes the entropy of a dataset.\n",
        "\n",
        "    Entropy is a measure of impurity or disorder in the dataset. It is calculated\n",
        "    using the formula:\n",
        "        H(y) = - Î£ (p * log2(p)) for all classes in y\n",
        "    where `p` is the probability of each class in the dataset.\n",
        "\n",
        "    Args:\n",
        "        y (ndarray): A 1D NumPy array containing class labels.\n",
        "\n",
        "    Returns:\n",
        "        float: The entropy value, representing the impurity of the dataset.\n",
        "\n",
        "    Steps:\n",
        "        1. Identify unique class labels and their counts in the dataset.\n",
        "        2. Compute the probability of each class.\n",
        "        3. Apply the entropy formula to calculate the impurity.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)  # Find unique values and their counts\n",
        "    probabilities = counts / len(y)  # Calculate probabilities for each class\n",
        "    entropy_value = -np.sum([p * np.log2(p) for p in probabilities if p > 0])  # Apply the entropy formula\n",
        "    return entropy_value\n"
      ],
      "metadata": {
        "id": "BjBssB_Rt6g_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(data,feature_index,threshold):\n",
        "  \"\"\"\n",
        "     Splits the dataset into two subsets based on a given feature and threshold.\n",
        "\n",
        "     This function divides the dataset into two parts:\n",
        "        - The \"left\" subset contains rows where the feature value is less than\n",
        "          or equal to the specified threshold.\n",
        "        - The \"right\" subset contains rows where the feature value is greater\n",
        "          than the threshold.\n",
        "\n",
        "     Args:\n",
        "        data (ndarray): A NumPy array representing the dataset, where each row\n",
        "                        is a data sample, and columns are features.\n",
        "        feature_index (int): The index of the feature to split on.\n",
        "        threshold (float): The threshold value used for splitting.\n",
        "\n",
        "     Returns:\n",
        "        tuple: A tuple containing two NumPy arrays:\n",
        "            - left (ndarray): The subset of data where the feature value <= threshold.\n",
        "            - right (ndarray): The subset of data where the feature value > threshold.\n",
        "\n",
        "     Steps:\n",
        "        1. Iterate through each row in the dataset.\n",
        "        2. Compare the value of the specified feature with the threshold.\n",
        "        3. Assign rows to the left or right subset based on the comparison.\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "  left=np.array( [ row for row in data if row[feature_index]  <= threshold] )  # Subset where feature <= threshold\n",
        "  right=np.array( [ row for row in data if row[feature_index] > threshold] ) # Subset where feature > threshold\n",
        "\n",
        "  return left,right\n",
        ""
      ],
      "metadata": {
        "id": "DQToYgoQrPLQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split():\n",
        "  \"\"\"\n",
        "    Determines the best feature and threshold to split the dataset for a decision tree.\n",
        "\n",
        "    This function evaluates all possible splits for each feature in the dataset by iterating\n",
        "    through each feature and its unique values as potential thresholds. It calculates the\n",
        "    information gain for each split and selects the one that results in the highest information gain.\n",
        "\n",
        "    Returns:\n",
        "        best_feature_index (int): Index of the feature that provides the best split.\n",
        "        best_threshold (float): Threshold value of the best feature for splitting.\n",
        "\n",
        "    Steps:\n",
        "        1. Initialize variables to track the best information gain, feature, and threshold.\n",
        "        2. Loop through each feature in the dataset.\n",
        "        3. For each feature, determine its unique values as potential split thresholds.\n",
        "        4. Split the data based on each threshold and calculate the information gain.\n",
        "        5. Update the best split if the current split results in a higher information gain.\n",
        "        6. Return the feature index and threshold for the best split.\n",
        "\n",
        "    Notes:\n",
        "        - This function assumes the last column in the dataset is the target label.\n",
        "        - Skips invalid splits where one of the subsets is empty.\n",
        "    \"\"\"\n",
        "  best_gain_information=0\n",
        "  best_threshold=0\n",
        "  best_feature_index=0\n",
        "  x=data[:,:-1]\n",
        "  y=data[: ,-1]\n",
        "  num_feature=x.shape[1]\n",
        "  for feature_index in range(num_feature):\n",
        "    thresholds=np.unique(data[:,feature_index])\n",
        "    for threshold in thresholds:\n",
        "        left,right = split(data,feature_index,threshold)\n",
        "        if len(left)==0 or len(right)==0 :\n",
        "          continue\n",
        "\n",
        "        gain=gain_information(data[: ,-1],left[:, -1],right[: ,-1])\n",
        "\n",
        "        if gain > best_gain_information:\n",
        "          best_gain_information = gain\n",
        "          best_feature_index=feature_index\n",
        "          best_threshold=threshold\n",
        "\n",
        "  return best_feature_index , best_threshold"
      ],
      "metadata": {
        "id": "eKql9qJsvxNO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gain_information(parent,left,right):\n",
        "    \"\"\"\n",
        "    Calculates the information gain from splitting a dataset.\n",
        "\n",
        "    Information gain measures the reduction in entropy after a dataset is split\n",
        "    into two subsets. It quantifies how much the uncertainty (impurity) of the\n",
        "    data is reduced by the split.\n",
        "\n",
        "    Args:\n",
        "        parent (ndarray): The labels of the original dataset before splitting.\n",
        "        left (ndarray): The labels of the subset where the feature value <= threshold.\n",
        "        right (ndarray): The labels of the subset where the feature value > threshold.\n",
        "\n",
        "    Returns:\n",
        "        float: The information gain achieved by splitting the dataset.\n",
        "\n",
        "    Formula:\n",
        "        Information Gain = Entropy(parent) -\n",
        "                           [ (|left| / |parent|) * Entropy(left) +\n",
        "                             (|right| / |parent|) * Entropy(right) ]\n",
        "\n",
        "    Steps:\n",
        "        1. Calculate the proportion of the left and right subsets relative to the parent dataset.\n",
        "        2. Compute the weighted average of the entropy for the left and right subsets.\n",
        "        3. Subtract the weighted entropy of the subsets from the parent's entropy to get the information gain.\n",
        "    \"\"\"\n",
        "\n",
        "    num_lfet=len(left)/len(parent)# Proportion of the left subset to the parent dataset\n",
        "    num_right=len(right)/len(parent)# Proportion of the right subset to the parent dataset\n",
        "    gain_information=entropy(parent)-((num_lfet*entropy(left))+(num_right*entropy(right)))  # Compute the information gain\n",
        "\n",
        "    return gain_information\n",
        "\n"
      ],
      "metadata": {
        "id": "asiS7tp3G9Ov"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(data, depth=0, max_depth=10):\n",
        "    \"\"\"\n",
        "    Builds a decision tree recursively using the input dataset.\n",
        "\n",
        "    The function splits the dataset into subsets based on the feature and\n",
        "    threshold that provide the highest information gain. The process continues\n",
        "    until one of the stopping criteria is met: maximum depth is reached or all\n",
        "    samples belong to the same class.\n",
        "\n",
        "    Args:\n",
        "        data (ndarray): A NumPy array where rows are samples and columns are\n",
        "                        features, with the last column being the target labels.\n",
        "        depth (int): The current depth of the tree. Defaults to 0.\n",
        "        max_depth (int): The maximum depth allowed for the tree. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        dict or int: A dictionary representing a node in the decision tree if\n",
        "                     the tree continues to split, or an integer representing the\n",
        "                     predicted class if the tree reaches a leaf node.\n",
        "\n",
        "    Steps:\n",
        "        1. Split the dataset into features (`X`) and target labels (`y`).\n",
        "        2. Count the number of samples per class to determine the majority class.\n",
        "        3. Check stopping criteria:\n",
        "           - If maximum depth is reached or all labels are identical, return\n",
        "             the majority class as a leaf node.\n",
        "        4. Find the best feature and threshold for splitting the data.\n",
        "        5. Split the data into left and right subsets.\n",
        "        6. Recursively build the left and right branches of the tree.\n",
        "        7. Return a node dictionary containing the splitting feature, threshold,\n",
        "           and pointers to the left and right subtrees.\n",
        "    \"\"\"\n",
        "    X, y = data[:, :-1], data[:, -1]\n",
        "    num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
        "    predicted_class = np.argmax(num_samples_per_class)\n",
        "\n",
        "    if depth >= max_depth or len(np.unique(y)) == 1:\n",
        "        return predicted_class\n",
        "\n",
        "    feature_index, threshold = best_split()\n",
        "    left, right = split(data, feature_index, threshold)\n",
        "\n",
        "    node = {\n",
        "        'feature_index': feature_index,\n",
        "        'threshold': threshold,\n",
        "        'left': build_tree(left, depth + 1, max_depth),\n",
        "        'right': build_tree(right, depth + 1, max_depth)\n",
        "    }\n",
        "    return node\n"
      ],
      "metadata": {
        "id": "JQZA6VlJpq73"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tree, sample):\n",
        "    \"\"\"\n",
        "    Makes a prediction by traversing the decision tree.\n",
        "\n",
        "    This function recursively traverses the decision tree, starting from the root.\n",
        "    At each node, it checks the feature value for the sample and decides whether to\n",
        "    move to the left or right branch based on the threshold. The process continues\n",
        "    until it reaches a leaf node, which contains the predicted class label.\n",
        "\n",
        "    Args:\n",
        "        tree (dict): A dictionary representing a node in the decision tree. If the node is a leaf,\n",
        "                     it contains the predicted class; otherwise, it contains the feature index,\n",
        "                     threshold, and the left and right branches (subtrees).\n",
        "        sample (ndarray): A single data sample (row of features) to be classified. The sample is\n",
        "                          an array of feature values.\n",
        "\n",
        "    Returns:\n",
        "        int: The predicted class label for the input sample.\n",
        "\n",
        "    Steps:\n",
        "        1. Check if the current node is a leaf (not a dictionary). If it is, return the class label.\n",
        "        2. If the current node is not a leaf, compare the sample's feature value with the threshold.\n",
        "        3. If the sample's feature value is less than or equal to the threshold, recursively call `predict` on the left subtree.\n",
        "        4. If the sample's feature value is greater than the threshold, recursively call `predict` on the right subtree.\n",
        "    \"\"\"\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    feature_index = tree['feature_index']\n",
        "    threshold = tree['threshold']\n",
        "\n",
        "    if sample[feature_index] <= threshold:\n",
        "        return predict(tree['left'], sample)\n",
        "    else:\n",
        "        return predict(tree['right'], sample)\n"
      ],
      "metadata": {
        "id": "oIX6BEaM6e2y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the decision tree\n",
        "tree = build_tree(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "2aunA47H6ezd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction\n",
        "prediction = predict(tree, [1, 0, 1])  # Example: Pointy, Not Round, Whiskers\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYjpDhuq7Ao8",
        "outputId": "f4508876-5f44-4392-adca-bf0e57c0de1b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2u5qjKB18uY6"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}